{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ceb170-6120-4a76-99ad-b9f573b5ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP extracted into folder: amazon_ml_submission\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"68e8d1d70b66d_student_resource.zip\"\n",
    "extract_path = \"amazon_ml_submission\"  # target folder\n",
    "\n",
    "# Create target folder if it doesn't exist\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "\n",
    "# Extract all files into this folder\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"ZIP extracted into folder: {extract_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4413ea0-e74c-4875-bfef-99b7dafb2c02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon_ml_submission/\n",
      "    student_resource/\n",
      "        Documentation_template.md\n",
      "        .DS_Store\n",
      "        README.md\n",
      "        sample_code.py\n",
      "        dataset/\n",
      "            sample_test_out.csv\n",
      "            sample_test.csv\n",
      "            test.csv\n",
      "            train.csv\n",
      "        src/\n",
      "            example.ipynb\n",
      "            utils.py\n",
      "            __pycache__/\n",
      "                utils.cpython-310.pyc\n",
      "                utils.cpython-37.pyc\n",
      "            .ipynb_checkpoints/\n",
      "                utils-checkpoint.py\n",
      "    __MACOSX/\n",
      "        ._student_resource\n",
      "        student_resource/\n",
      "            ._src\n",
      "            ._dataset\n",
      "            ._.DS_Store\n",
      "            ._sample_code.py\n",
      "            ._Documentation_template.md\n",
      "            ._README.md\n",
      "            dataset/\n",
      "                ._train.csv\n",
      "                ._sample_test.csv\n",
      "                ._sample_test_out.csv\n",
      "                ._test.csv\n",
      "            src/\n",
      "                ._utils.py\n",
      "                .___pycache__\n",
      "                ._example.ipynb\n",
      "                __pycache__/\n",
      "                    ._utils.cpython-37.pyc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your extracted folder\n",
    "folder_path = \"amazon_ml_submission\"\n",
    "\n",
    "# Walk through the folder and print files/subfolders\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    level = root.replace(folder_path, \"\").count(os.sep)\n",
    "    indent = \" \" * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = \" \" * 4 * (level + 1)\n",
    "    for f in files:\n",
    "        print(f\"{sub_indent}{f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180e3a89-f260-4308-9752-165a77ce49e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac35f9d-6db7-48c3-a46d-bba0c3b3e7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in ./.local/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.local/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.local/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/tljh/user/lib/python3.10/site-packages (from triton==3.4.0->torch) (65.6.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.10/site-packages (4.57.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.10/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.10/site-packages (11.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install transformers\n",
    "!pip install pandas numpy\n",
    "!pip install pillow\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17273ca1-e5f5-4833-bbd8-e4348f8c8c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install HuggingFace Transformers and EfficientNet\n",
    "!pip install torch torchvision transformers efficientnet_pytorch tqdm --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "177d1563-7009-40d8-84e8-d8d7089e0a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.10/site-packages (5.1.1)\n",
      "Requirement already satisfied: timm in ./.local/lib/python3.10/site-packages (1.0.20)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: pillow in ./.local/lib/python3.10/site-packages (11.3.0)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/tljh/user/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.local/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.local/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/tljh/user/lib/python3.10/site-packages (from triton==3.4.0->torch) (65.6.3)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.local/lib/python3.10/site-packages (from sentence-transformers) (4.57.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.local/lib/python3.10/site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/tljh/user/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/tljh/user/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/tljh/user/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision sentence-transformers timm tqdm pandas numpy pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e120588-c470-495f-bf0e-8ece00924527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-20 14:14:24.810178: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-20 14:14:24.854128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-20 14:14:25.903573: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KFold\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/__init__.py:15\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[1;32m     12\u001b[0m     export_optimized_onnx_model,\n\u001b[1;32m     13\u001b[0m     export_static_quantized_openvino_model,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m     CrossEncoder,\n\u001b[1;32m     17\u001b[0m     CrossEncoderModelCardData,\n\u001b[1;32m     18\u001b[0m     CrossEncoderTrainer,\n\u001b[1;32m     19\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_model, load_openvino_model\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_mixin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     cross_encoder_init_args_decorator,\n\u001b[1;32m     33\u001b[0m     cross_encoder_predict_rank_args_decorator,\n\u001b[1;32m     34\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/fit_mixin.py:19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/datasets/__init__.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/datasets/ParallelSentencesDataset.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[1;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:36\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Router\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mModule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/model_card.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/integrations/integration_utils.py:60\u001b[0m\n\u001b[1;32m     57\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2317\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module:\n\u001b[1;32m   2316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2317\u001b[0m         module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2318\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   2319\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2347\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:2345\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   2344\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2345\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# 1. Paths\n",
    "# ===============================\n",
    "base_path = \"student_resource/dataset\"\n",
    "train_path = os.path.join(base_path, \"train.csv\")\n",
    "test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Data\n",
    "# ===============================\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "print(f\"Train samples: {train.shape[0]}, Test samples: {test.shape[0]}\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Remove Outliers (IQR)\n",
    "# ===============================\n",
    "Q1 = train['price'].quantile(0.25)\n",
    "Q3 = train['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "train = train[(train['price'] >= lower_bound) & (train['price'] <= upper_bound)]\n",
    "print(f\"After IQR removal, train samples: {train.shape[0]}\")\n",
    "\n",
    "# ===============================\n",
    "# 4. Feature Engineering\n",
    "# ===============================\n",
    "def extract_numeric(text):\n",
    "    nums = re.findall(r'\\d+', str(text))\n",
    "    return max([int(n) for n in nums], default=1)\n",
    "\n",
    "# Quantity / IPQ\n",
    "train['quantity'] = train['catalog_content'].apply(extract_numeric)\n",
    "test['quantity'] = test['catalog_content'].apply(extract_numeric)\n",
    "\n",
    "# Text stats\n",
    "train['text_len'] = train['catalog_content'].apply(lambda x: len(str(x)))\n",
    "test['text_len'] = test['catalog_content'].apply(lambda x: len(str(x)))\n",
    "\n",
    "train['num_count'] = train['catalog_content'].apply(lambda x: len(re.findall(r'\\d+', str(x))))\n",
    "test['num_count'] = test['catalog_content'].apply(lambda x: len(re.findall(r'\\d+', str(x))))\n",
    "\n",
    "# Brand extraction\n",
    "train['brand'] = train['catalog_content'].apply(lambda x: str(x).split()[0])\n",
    "test['brand'] = test['catalog_content'].apply(lambda x: str(x).split()[0])\n",
    "le = LabelEncoder()\n",
    "train['brand_enc'] = le.fit_transform(train['brand'])\n",
    "test['brand_enc'] = le.transform(test['brand'])\n",
    "\n",
    "# Word count and average word length\n",
    "train['word_count'] = train['catalog_content'].apply(lambda x: len(str(x).split()))\n",
    "test['word_count'] = test['catalog_content'].apply(lambda x: len(str(x).split()))\n",
    "train['avg_word_len'] = train['catalog_content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split())>0 else 0)\n",
    "test['avg_word_len'] = test['catalog_content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split())>0 else 0)\n",
    "\n",
    "# ===============================\n",
    "# 5. Text Embeddings\n",
    "# ===============================\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "train_text_embeddings = embedder.encode(train['catalog_content'].tolist(), batch_size=64, show_progress_bar=True)\n",
    "test_text_embeddings = embedder.encode(test['catalog_content'].tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# ===============================\n",
    "# 6. Combine Features\n",
    "# ===============================\n",
    "numeric_features = ['quantity','text_len','num_count','brand_enc','word_count','avg_word_len']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_numeric = scaler.fit_transform(train[numeric_features])\n",
    "test_numeric = scaler.transform(test[numeric_features])\n",
    "\n",
    "X_train_features = np.hstack([train_text_embeddings, train_numeric])\n",
    "X_test_features = np.hstack([test_text_embeddings, test_numeric])\n",
    "\n",
    "# ===============================\n",
    "# 7. Target Transformation\n",
    "# ===============================\n",
    "y = np.log1p(train['price'])  # log-transform stabilizes high prices\n",
    "\n",
    "# ===============================\n",
    "# 8. Custom SMAPE for LightGBM\n",
    "# ===============================\n",
    "def smape_lgb(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    y_pred = np.expm1(y_pred)  # revert log1p\n",
    "    y_true_exp = np.expm1(y_true)\n",
    "    denominator = (np.abs(y_true_exp) + np.abs(y_pred)) / 2\n",
    "    denominator[denominator == 0] = 1e-6\n",
    "    grad = (y_pred - y_true_exp) / denominator\n",
    "    hess = np.ones_like(grad)\n",
    "    return grad, hess\n",
    "\n",
    "def smape_metric(y_pred, dataset):\n",
    "    y_true = np.expm1(dataset.get_label())\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "    return 'SMAPE', smape, False\n",
    "\n",
    "# ===============================\n",
    "# 9. Train LightGBM with K-Fold\n",
    "# ===============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "preds = np.zeros(X_test_features.shape[0])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features)):\n",
    "    X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train','valid'],\n",
    "        fobj=smape_lgb,\n",
    "        feval=smape_metric,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    preds += np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration)) / kf.n_splits\n",
    "\n",
    "# ===============================\n",
    "# 10. Save Submission\n",
    "# ===============================\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'],\n",
    "    'price': preds.clip(min=1)  # ensure positive prices\n",
    "})\n",
    "submission_path = os.path.join(base_path, \"test1_out.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Submission saved as {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d6ebe4-26a1-43f4-9cbe-9630fdbdef34",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[1;32m      8\u001b[0m kf \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43mX_test_features\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(kf\u001b[38;5;241m.\u001b[39msplit(X_train_features)):\n\u001b[1;32m     12\u001b[0m     X_tr, X_val \u001b[38;5;241m=\u001b[39m X_train_features[train_idx], X_train_features[val_idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test_features' is not defined"
     ]
    }
   ],
   "source": [
    "def smape_metric(y_pred, dataset):\n",
    "    y_true = np.expm1(dataset.get_label())\n",
    "    y_pred = np.expm1(y_pred)\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true))) * 100\n",
    "    return 'SMAPE', smape, False\n",
    "\n",
    "# Training loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "preds = np.zeros(X_test_features.shape[0])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features)):\n",
    "    X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'mae',  # use MAE as the loss\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train','valid'],\n",
    "        feval=smape_metric,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    preds += np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration)) / kf.n_splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b08b41-3957-4c54-88c2-dcfdd23ab347",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,  # train additional rounds\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train','valid'],\n",
    "    feval=smape_metric,\n",
    "    init_model=model,  # continue from existing model\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c670f3a-f3e9-4fef-bcd2-4a38a2746add",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train, free_raw_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806156b-0409-4d64-adca-2d312e1c7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep raw data\n",
    "lgb_train = lgb.Dataset(X_tr, label=y_tr, free_raw_data=False)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train, free_raw_data=False)\n",
    "\n",
    "# Continue training\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=1000,  # additional rounds\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train','valid'],\n",
    "    feval=smape_metric,\n",
    "    init_model=model,       # continue from previous model\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ceb76-1560-4a2b-8c8a-a0a65ecf0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "params['learning_rate'] = 0.01\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=2000,  # more rounds with smaller learning rate\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train','valid'],\n",
    "    feval=smape_metric,\n",
    "    init_model=model,\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=200), lgb.log_evaluation(100)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e2307c-31d8-4d18-a6ee-01c55fbf065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_val - np.expm1(model.predict(X_val_features, num_iteration=model.best_iteration))\n",
    "res_model = LGBMRegressor(...).fit(X_train_features, residuals)\n",
    "preds += res_model.predict(X_test_features) / n_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90f502-983b-4bc6-87a8-b7c1302eef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation set\n",
    "val_preds = np.expm1(model.predict(X_val, num_iteration=model.best_iteration))\n",
    "\n",
    "# Residuals = true values - predicted\n",
    "residuals = y_val - val_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134262d6-8fad-41db-a1b9-be908c1f60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals on training fold\n",
    "train_preds = np.expm1(model.predict(X_tr, num_iteration=model.best_iteration))\n",
    "residuals_tr = y_tr - train_preds\n",
    "\n",
    "# Train residual model on X_tr\n",
    "res_model.fit(X_tr, residuals_tr)\n",
    "\n",
    "# Predict on test set\n",
    "res_preds = res_model.predict(X_test_features)\n",
    "\n",
    "# Combine with original predictions\n",
    "final_preds = np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration)) + res_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e056f-38fe-4d42-a4b8-4738075cb1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For fold i\n",
    "X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Predictions on the training fold, not validation\n",
    "train_preds = np.expm1(model.predict(X_tr, num_iteration=model.best_iteration))\n",
    "\n",
    "# Residuals = true - predicted on training fold\n",
    "residuals_tr = y_tr - train_preds\n",
    "\n",
    "# Fit residual model on the training fold\n",
    "res_model.fit(X_tr, residuals_tr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19defc2d-2133-46ea-9be2-1639b42fbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = np.expm1(model.predict(X_train_features, num_iteration=model.best_iteration))\n",
    "residuals = y - train_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5f5c2-2694-400f-bd82-9a32448ce2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = LGBMRegressor(\n",
    "    n_estimators=2000,\n",
    "    learning_rate=0.01,\n",
    "    num_leaves=64,\n",
    "    random_state=42\n",
    ")\n",
    "res_model.fit(X_train_features, residuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408dc7ed-26b7-455c-a4bc-574c9f54c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_orig = np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration))\n",
    "res_preds = res_model.predict(X_test_features)\n",
    "final_preds = test_preds_orig + res_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c330bdc-d127-4123-aa38-f81c4b31e898",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = np.clip(final_preds, 1, np.percentile(final_preds, 99.9))\n",
    "final_preds = np.round(final_preds, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3881b786-99b9-4eb2-bbe1-2e5b729677e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model2 = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "model2.fit(X_train_features, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d4a35-6bd7-425f-8f43-3abe856e338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Make sure test_df is your test dataset containing sample_id\n",
    "submission = pd.DataFrame({\n",
    "    \"sample_id\": test_df['sample_id'],  \n",
    "    \"price\": final_preds\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"✅ Submission file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f2633-4ed4-4fc3-b013-cad0b168b67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_model1 = final_preds  # From your first model\n",
    "preds_model2 = model2.predict(X_test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497a835c-6315-4324-ac27-cccf49297db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ensemble_preds = 0.6*preds_model1 + 0.4*preds_model2\n",
    "final_ensemble_preds = np.clip(final_ensemble_preds, 1, np.percentile(final_ensemble_preds, 99.9))\n",
    "final_ensemble_preds = np.round(final_ensemble_preds, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ab05ca-1b8a-43fa-85af-e70026b1b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"price\"] = final_ensemble_preds\n",
    "submission.to_csv(\"final_submission_ensemble.csv\", index=False)\n",
    "print(\"✅ Ensemble submission saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b847d255-a53c-4662-add6-68adf6d6d3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"student_resource/dataset/test.csv\")\n",
    "import pandas as pd\n",
    "\n",
    "# Load the test file (adjust path if needed)\n",
    "test_df = pd.read_csv(\"student_resource/dataset/test.csv\")  \n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],  # keep the IDs from test set\n",
    "    \"price\": final_ensemble_preds                 # your final predictions\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1384add6-6645-4624-b197-ea1104ea14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in the required format\n",
    "submission_df.to_csv(\"test.csv\", index=False)\n",
    "print(\"Submission file saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553778b-2a0a-4104-ad20-d0520b3bb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.log1p(y_train)  # log(1 + price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b5e9e-cda7-4417-b6c6-4dcd28ff0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load test file\n",
    "test_df = pd.read_csv(\"student_resource/dataset/test.csv\")\n",
    "\n",
    "# Make sure final_ensemble_preds is same length as test set\n",
    "assert len(final_ensemble_preds) == len(test_df), \"Predictions length mismatch!\"\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": final_ensemble_preds\n",
    "})\n",
    "\n",
    "# Save CSV\n",
    "submission_df.to_csv(\"final_submission_ensemble.csv\", index=False)\n",
    "print(\"✅ Ensemble submission saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e91d88-6cf7-4a62-b22b-be44423ff1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM predictions in log scale\n",
    "preds_lgb = model.predict(X_test_features, num_iteration=model.best_iteration)\n",
    "\n",
    "# CatBoost predictions in log scale if needed\n",
    "preds_cb = np.log1p(model2.predict(X_test_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3f9b5-d908-481a-8d8c-12fdfe473686",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds_log = 0.6 * preds_lgb + 0.4 * preds_cb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92dd52-1c47-4476-82ec-51de7b9b9deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ensemble_preds = np.expm1(final_preds_log)\n",
    "final_ensemble_preds = np.clip(final_ensemble_preds, 1, np.percentile(final_ensemble_preds, 99.9))\n",
    "final_ensemble_preds = np.round(final_ensemble_preds, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b0a525-8893-48e9-8d68-18f74668e397",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test_df[\"sample_id\"],\n",
    "    \"price\": final_ensemble_preds\n",
    "})\n",
    "\n",
    "# Save as file1.csv\n",
    "submission_df.to_csv(\"file1.csv\", index=False)\n",
    "print(\"✅ Ensemble submission saved as file1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0f5c7-ed0c-407e-bdff-3e7b5bc98ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_raw = model.predict(X_test_features, num_iteration=model.best_iteration)\n",
    "print(np.min(preds_raw), np.max(preds_raw))  # are they negative?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd29137-4c23-4ac4-ae94-0d8cc6f95133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally clip only extreme prices\n",
    "train['price'] = train['price'].clip(1, train['price'].quantile(0.99))\n",
    "y = np.log1p(train['price'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cd961-45b4-4f5b-8463-a6642ed4cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_raw = model.predict(X_test_features, num_iteration=model.best_iteration)\n",
    "print(np.min(preds_raw), np.max(preds_raw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad52996-0b47-4906-a79e-c4907291fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 0. Imports\n",
    "# ===============================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===============================\n",
    "# 1. Paths\n",
    "# ===============================\n",
    "base_path = \"student_resource/dataset\"\n",
    "train_path = os.path.join(base_path, \"train.csv\")\n",
    "test_path = os.path.join(base_path, \"test.csv\")\n",
    "\n",
    "# ===============================\n",
    "# 2. Load Data\n",
    "# ===============================\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "print(f\"Train samples: {train.shape[0]}, Test samples: {test.shape[0]}\")\n",
    "\n",
    "# ===============================\n",
    "# 3. Feature Engineering\n",
    "# ===============================\n",
    "\n",
    "def extract_numeric(text):\n",
    "    nums = re.findall(r'\\d+', str(text))\n",
    "    return max([int(n) for n in nums], default=1)\n",
    "\n",
    "# Quantity / IPQ\n",
    "train['quantity'] = train['catalog_content'].apply(extract_numeric)\n",
    "test['quantity'] = test['catalog_content'].apply(extract_numeric)\n",
    "\n",
    "# Text statistics\n",
    "for df in [train, test]:\n",
    "    df['text_len'] = df['catalog_content'].apply(lambda x: len(str(x)))\n",
    "    df['num_count'] = df['catalog_content'].apply(lambda x: len(re.findall(r'\\d+', str(x))))\n",
    "    df['word_count'] = df['catalog_content'].apply(lambda x: len(str(x).split()))\n",
    "    df['avg_word_len'] = df['catalog_content'].apply(lambda x: np.mean([len(w) for w in str(x).split()]) if len(str(x).split())>0 else 0)\n",
    "    df['brand'] = df['catalog_content'].apply(lambda x: str(x).split()[0])\n",
    "\n",
    "# Encode brands\n",
    "le = LabelEncoder()\n",
    "train['brand_enc'] = le.fit_transform(train['brand'])\n",
    "test['brand_enc'] = le.transform(test['brand'])\n",
    "\n",
    "numeric_features = ['quantity','text_len','num_count','word_count','avg_word_len','brand_enc']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_numeric = scaler.fit_transform(train[numeric_features])\n",
    "test_numeric = scaler.transform(test[numeric_features])\n",
    "\n",
    "# ===============================\n",
    "# 4. Text Embeddings\n",
    "# ===============================\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "train_text_embeddings = embedder.encode(train['catalog_content'].tolist(), batch_size=64, show_progress_bar=True)\n",
    "test_text_embeddings = embedder.encode(test['catalog_content'].tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Combine numeric + embeddings\n",
    "X_train_features = np.hstack([train_text_embeddings, train_numeric])\n",
    "X_test_features = np.hstack([test_text_embeddings, test_numeric])\n",
    "y = train['price'].values  # use raw price, not log1p\n",
    "\n",
    "# ===============================\n",
    "# 5. K-Fold Training LightGBM\n",
    "# ===============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "lgb_preds = np.zeros(X_test_features.shape[0])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features)):\n",
    "    X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 128,\n",
    "        'max_depth': -1,\n",
    "        'metric': 'mae',\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train','valid'],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    \n",
    "    lgb_preds += model.predict(X_test_features, num_iteration=model.best_iteration) / kf.n_splits\n",
    "\n",
    "# ===============================\n",
    "# 6. CatBoost Training\n",
    "# ===============================\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=200\n",
    ")\n",
    "cat_model.fit(X_train_features, y)\n",
    "\n",
    "cat_preds = cat_model.predict(X_test_features)\n",
    "\n",
    "# ===============================\n",
    "# 7. Ensemble (Stacking)\n",
    "# ===============================\n",
    "final_preds = 0.6*lgb_preds + 0.4*cat_preds\n",
    "final_preds = np.clip(final_preds, 1, np.percentile(final_preds, 99.9))\n",
    "final_preds = np.round(final_preds, 2)\n",
    "\n",
    "# ===============================\n",
    "# 8. Save Submission\n",
    "# ===============================\n",
    "submission = pd.DataFrame({\n",
    "    'sample_id': test['sample_id'],\n",
    "    'price': final_preds\n",
    "})\n",
    "submission_path = os.path.join(base_path, \"final_submission_ensemble.csv\")\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Submission saved: {submission_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9410d6c-645a-4e65-9b89-64111af3e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=5000,\n",
    "    valid_sets=[lgb_train, lgb_val],\n",
    "    valid_names=['train','valid'],\n",
    "    feval=smape_metric,  # your custom SMAPE\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=200)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722be87-cbe2-4b11-8d88-d6cce5ffd365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_metric_log(y_pred, dataset):\n",
    "    y_true = dataset.get_label()\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    denominator[denominator == 0] = 1e-6\n",
    "    smape = np.mean(2 * np.abs(y_pred - y_true) / denominator) * 100\n",
    "    return 'SMAPE', smape, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b04649-0bac-4e3f-b527-a1394a626401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ===============================\n",
    "# 1. Prepare K-Fold\n",
    "# ===============================\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "preds = np.zeros(X_test_features.shape[0])  # for test predictions\n",
    "\n",
    "# ===============================\n",
    "# 2. K-Fold Training Loop\n",
    "# ===============================\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "    X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train','valid'],\n",
    "        feval=smape_metric_log,  # use log-space SMAPE\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100), lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Predict on test set and average over folds\n",
    "    preds += np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration)) / kf.n_splits\n",
    "\n",
    "# ===============================\n",
    "# 3. Clip and round predictions\n",
    "# ===============================\n",
    "final_preds = np.clip(preds, 1, np.percentile(preds, 99.9))\n",
    "final_preds = np.round(final_preds, 2)\n",
    "\n",
    "# ===============================\n",
    "# 4. Save submission\n",
    "# ===============================\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test['sample_id'],\n",
    "    \"price\": final_preds\n",
    "})\n",
    "\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"✅ Submission saved as final_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4be3f8-66a5-43e5-87fb-020a65e787e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(train['price'].values)  # as array\n",
    "\n",
    "# Then use array indexing instead of .iloc\n",
    "y_tr, y_val = y[train_idx], y[val_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953a41a-d1a4-4337-b4f6-d84b3f17637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "preds = np.zeros(X_test_features.shape[0])\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_features)):\n",
    "    print(f\"Training fold {fold+1}...\")\n",
    "    \n",
    "    X_tr, X_val = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_val = lgb.Dataset(X_val, label=y_val, reference=lgb_train)\n",
    "    \n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'metric': 'mae',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'max_depth': -1,\n",
    "        'seed': 42,\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=5000,\n",
    "        valid_sets=[lgb_train, lgb_val],\n",
    "        valid_names=['train','valid'],\n",
    "        feval=smape_metric_log,   # your custom SMAPE\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100),\n",
    "                   lgb.log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Predict on test set and average over folds\n",
    "    preds += np.expm1(model.predict(X_test_features, num_iteration=model.best_iteration)) / kf.n_splits\n",
    "\n",
    "# Clip extreme predictions\n",
    "final_preds = np.clip(preds, 1, np.percentile(preds, 99.9))\n",
    "final_preds = np.round(final_preds, 2)\n",
    "\n",
    "# Create submission\n",
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test['sample_id'],\n",
    "    \"price\": final_preds\n",
    "})\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"✅ Submission saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c337f8-74d9-4c73-9ebb-4c6ed69fd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"sample_id\": test['sample_id'],\n",
    "    \"price\": final_preds\n",
    "})\n",
    "submission_df.to_csv(\"final2.csv\", index=False)\n",
    "print(\"✅ Submission saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d60f4-b527-47fb-908d-b0376c9602bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
